HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection Binny Mathew1*, Punyajoy Saha1*, Seid Muhie Yimam2 Chris Biemann2, Pawan Goyal1, Animesh Mukherjee1 1Indian Institute of Technology, Kharagpur, India 2Universit ¨at Hamburg, Germany binnymathew@iitkgp.ac.in, punyajoys@iitkgp.ac.in, yimam@informatik.uni-hamburg.de biemann@informatik.uni-hamburg.de, pawang@cse.iitkgp.ac.in, animeshm@cse.iitkgp.ac.in Abstract Hate speech is a challenging issue plaguing the online so- cial media.While better models for hate speech detection are continuously being developed, there is little research on the bias andinterpretability aspects of hate speech.In this paper, we introduce HateXplain, the ﬁrst benchmark hate speech dataset covering multiple aspects of the issue.

Each post in our dataset is annotated from three different perspectives: the basic, commonly used 3-class classiﬁcation (i.e., hate, offen- sive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. We utilize existing state-of-the-art models and ob- serve that even models that perform very well in classiﬁcation do not score high on explainability metrics like model plau- sibility andfaithfulness.We also observe that models, which utilize the human rationales for training, perform better in re- ducing unintended bias towards target communities.We have made our code and dataset public1for other researchers2.Introduction The increase in online hate speech is a major cultural threat, as it already resulted in crime against minorities, see e.g.(Williams et al.2020).

To tackle this issue, there has been a rising interest in hate speech detection to expose and regulate this phenomenon. Several hate speech datasets (Ousidhoum et al.2019; Qian et al.2019b; de Gibert et al.2018; Sanguinetti et al.2018), models (Zhang, Robinson, and Tepper 2018; Mishra et al.2018; Qian et al.2018b,a), and shared tasks (Basile et al.2019; Bosco et al.2018) have been made available in the recent years by the community, towards the development of automatic hate speech detection.While many models have claimed to achieve state-of- the-art performance on some datasets, they fail to gener- alize (Arango, P ´erez, and Poblete 2019; Gr ¨ondahl et al.*Equal Contribution Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org).All rights reserved.1https://github.com/punyajoy/HateXplain 2Disclaimer: The article contains material that many will ﬁnd offensive or hateful; however this cannot be avoided owing to the nature of the work.2018).

The models may classify comments that refer to cer- tain commonly-attacked identities (e.g., gay, black, muslim) as toxic without the comment having any intention of be- ing toxic (Dixon et al. 2018; Borkan et al.2019).A large prior on certain trigger vocabulary leads to biased predic- tions that may discriminate against particular groups who are already the target of such abuse (Sap et al.2019; David- son, Bhattacharya, and Weber 2019).Another issue with the current methods is the lack of explanation about the deci- sions made.With hate speech detection models becoming increasingly complex, it is getting difﬁcult to explain their decisions (Goodfellow, Bengio, and Courville 2016).Laws such as General Data Protection Regulation (GDPR (Coun- cil 2016)) in Europe have recently established a “right to ex- planation”.This calls for a shift in perspective from perfor- mance based models to interpretable models.

In our work, we approach model explainability by learning the target classiﬁcation and the reasons for the human decision jointly, and also to their mutual improvement. We therefore have compiled a dataset that covers mul- tiple aspects of hate speech.We collect posts from Twit- ter3and Gab4, and ask Amazon Mechanical Turk (MTurk) workers to annotate these posts to cover three facets.In ad- dition to classifying each post into hate, offensive, or nor- mal speech, annotators are asked to select the target com- munities mentioned in the post.Subsequently, the annota- tors are asked to highlight parts of the text that could jus- tify their classiﬁcation decision5.The notion of justiﬁcation, here modeled as ‘human attention’, is very broad with many possible realizations (Lipton 2018; Doshi-Velez 2017).In this paper, we speciﬁcally focus on using rationales, i.e., snippets of text from a source text that support a particular categorization.

Such rationales have been used in common- sense explanations (Rajani et al. 2019), e-SNLI (Camburu et al.2018) and several other tasks (DeYoung et al.2020).If these rationales are good reasons for decisions, then mod- els guided towards these in training could be made more human-decision-taking-like.Consider the examples in Table 1.The ﬁrst row shows 3https://twitter.com/ 4https://gab.com/ 5In case the post is classiﬁed as normal, the annotators does not need to highlight any span.TheThirty-FifthAAAIConferenceonArtificial Intelligence(AAAI-21) 14867 Model Text Label Human Annotator The jews are again using holohoax as an excuse to spread their agenda .Hilter should have eradicated them HS CNN-GRU The jews are again using holohoax asan excuse to spread their agenda .

Hilter should have eradicated them HS BiRNN Thejews areagain using holohoax as an excuse to spread their agenda .Hilter should have eradicated them HS BiRNN-Attn The jews are again using holohoax as an excuse tospread their agenda . Hilter should have eradicated them HS BiRNN-HateXplain The jews are again using holohoax as an excuse to spread their agenda .Hilter should have eradicated them HS BERT Thejews are again using holohoax as an excuse to spread their agenda .Hilter should have eradicated them OF BERT-HateXplain The jews areagain using holohoax as an excuse to spread their agenda .Hilter should have eradicated them OF Table 1: Example of the rationales predicted by different models compared to human annotators.The bold part marks tokens that the human annotator and model found important for the prediction.The underlined part marks tokens which the model found important, but the human annotators did not.

the tokens (‘rationales’) that were selected by human an- notators which they believe are important for the classiﬁ- cation. The next six rows show the important tokens (using LIME (Ribeiro, Singh, and Guestrin 2016)), which helped various models in the classiﬁcation.We observe that even when the model is making the correct prediction (hate speech – HS in this case), the reason (‘rationales’) for this varies across models.In case of BERT, we observe that it at- tends to several of the tokens that human annotators deemed important, but assigns the wrong label (offensive speech - OF).In summary, we introduce HateXplain, the ﬁrst bench- mark dataset for hate speech with word and phrase level span annotations that capture human rationales for the label- ing.Using MTurk, we collect a large dataset of around 20K posts and annotate them to cover three aspects of each post.

We use several models on this dataset and observe that while they show a good model performance, they do not fare well in terms of model interpretability/explainability. We also ob- serve that providing these rationales as input during training helps in improving a model’s performance and reducing the unintended bias.We believe that this dataset would serve as a fundamental source for the future hate speech research.Related Works Hate Speech The public expression of hate speech affects the deval- uation of minority members (Greenberg and Pyszczynski 1985) and such frequent and repetitive exposure to hate speech could increase an individual’s outgroup prejudice (Soral, Bilewicz, and Winiewski 2018).Real world violent events could also lead to increased hate speech in online space (Olteanu et al.2018).To tackle this, various methods have been proposed for hate speech detection (Burnap and Williams 2016; Ribeiro et al.2018; Zhang, Robinson, and Tepper 2018; Qian et al.2018a).

The recent interest in hate speech research has led to the release of datasets in multiple languages (Ousidhoum et al. 2019; Sanguinetti et al.2018) along with different computational approaches to combat online hate (Qian et al.2019a; Mathew et al.2019b; Aluru et al.2020).A recurrent issue with the majority of previous research is that many of them tend to conﬂate hate speech and abu-sive/offensive6language (Davidson et al.2017).Some of the works have combined offensive and hate language under a single concept, while very few works, such as (Davidson et al.2017; Founta et al.2018) and Van Huynh et al.(2019) have attempted to separate offensive from hate speech.We argue that this, although subjective, is an important aspect as there are lots of messages that are offensive but do not qual- ify as hate speech.For example, consider the word ‘nigga’.The word is used everyday in online language by the African American community (Vigna et al.2017).

Similarly, words likehoeandbitch are used commonly in rap lyrics. Such language is prevalent on social media (Wang et al.2014) and any hate speech detection system should include these for the system to be usable.To this end, we have assumed that a given text can belong to one of the three classes: hate, offensive, normal.We have adopted the classes based on the work of Davidson et al.(2017).Table 2 provides a compari- son between some hate speech datasets.Explainability/Interpretability Zaidan, Eisner, and Piatko (2007) introduced the concept of using rationales, in which human annotators would high- light a span of text that could support their labeling decision.The authors utilized these enriched rationale annotation on a smaller set of training data, which helped to improve sen- timent classiﬁcation.Yessenalina, Choi, and Cardie (2010) built on this work and developed methods that automatically generate rationales.

Lei, Barzilay, and Jaakkola (2016) also proposed an encoder-generator framework, which provides quality rationales without any annotations. In our paper, we utilize the concept of rationales and pro- vide the ﬁrst benchmark hate speech dataset with human level explanations.We have made our model and dataset public1for other researchers.Dataset Collection And Annotation Strategies In this section, we provide the annotation strategies we have followed, the dataset selection approaches used, and the statistics of the collected dataset.6We have used the terms offensive and abusive interchangeably in our paper as they are arguably very similar (Founta et al.2018).14868 Dataset Labels Total Size Language Target Labels?Rationales?Waseem and Hovy (2016) racist, sexist, normal 16,914 English   Davidson et al.(2017) Hate Speech, Offensive, Normal 24,802 English   Founta et al.(2018) Abusive, Hateful, Normal, Spam 80,000 English   Ousidhoum et al.

(2019) Labels for ﬁve different aspects 13,000 English, French, Arabic X  HateXplain (Ours) Hate Speech, Offensive, Normal 20,148 English X X Table 2: Comparison of different hate speech datasets. Dataset Sampling We collect our dataset from sources where previous stud- ies on hate speech have been conducted: Twitter (Davidson et al.2017; Fortuna and Nunes 2018) and Gab (Lima et al.2018; Mathew et al.2020; Zannettou et al.2018).Follow- ing the existing literature, we build a corpus of posts (tweets and gab posts) using lexicons.We combined the lexicon set provided by Davidson et al.(2017), Ousidhoum et al.(2019), and Mathew et al.(2019a) to generate a single lexi- con.For Twitter, we ﬁlter the tweets from the 1% randomly collected tweets in the time period Jan-2019 to Jun-2020.In case of Gab, we use the dataset provided by Mathew et al.(2019a).We do not consider reposts and remove duplicates.

We also ensure that the posts do not contain links, pictures, or videos as they indicate additional information that might not be available to the annotators. However, we do not ex- clude the emojis from the text as they might carry important information for the hate and offensive speech labeling task.The posts were anonymized by replacing the usernames with <user> token.Annotation Procedure We use Amazon Mechanical Turk (MTurk) workers for our annotation task.Each post in our dataset contains three types of annotations.First, whether the text is a hate speech, of- fensive speech, or normal.Second, the target communities in the text.Third, if the text is considered as hate speech, or offensive by majority of the annotators, we further ask the annotators to annotate parts of the text, which are words or phrases that could be a potential reason for the given anno- tation.These additional span annotations allow us to further explore how hate or offensive speech manifests itself.

Target Group Annotation The primary goal of the an- notation task is to determine whether a given text is hate- ful, offensive, or neither of the two, i.e. normal.As noted above, we also get span annotations as reasons for the label assigned to a post (hateful or offensive).To further enrich the dataset, we ask the workers to decide the groups that the hate/offensive speech is targeting.We included target groups based on Race, Religion, Gender, Sexual Orientation etc.Annotation Instructions And Design Of The Interface Before starting the annotation task, workers are explicitly warned that the annotation task displays some hateful or offensive content.We prepare instructions for workers that clearly explain the goal of the annotation task, how to anno- tate spans and also include a deﬁnition for each category.

We provided multiple examples with classiﬁcation, target com-Twitter Gab Total Hateful 708 5,227 5,935 Offensive 2,328 3,152 5,480 Normal 5,770 2,044 7,814 Undecided 249 670 919 Total 9,055 11,093 20,148 Table 3: Dataset details. “Undecided” refers to the cases where all the three annotators chose a different class.munity and span annotations to help the annotators under- stand the task.To further ensure high quality dataset, we use built-in MTurk qualiﬁcation requirements, namely the HIT Approval Rate (95%) for all Requesters’ HITs and the Num- ber of HITs Approved (5,000) requirements.Dataset Creation Steps For the dataset creation, we ﬁrst conducted a pilot annotation study followed by the main annotation task.Pilot annotation: In the pilot task, each annotator was provided with 20 posts and they were required to do the hate/offensive speech classiﬁcation as well as identify the target community (if any).

In order to have a clear under- standing of the task, they were provided with multiple exam- ples along with explanations for the labelling process. The main purpose of the pilot task was to shortlist those anno- tators who were able to do the classiﬁcation accurately.We also collected feedback from annotators to improve the main annotation task.A total of 621 annotators took part in the pi- lot task.Out of these, 253 were selected for the main task.Main annotation: After the pilot annotation, once we had ascertained the quality of the annotators, we started with the main annotation task.In each round, we would select a batch of around 200 posts.Each post was annotated by three annotators, then majority voting was applied to decide the ﬁnal label.The ﬁnal dataset is composed of 9,055 posts from Twitter and 11,093 posts from Gab.Table 3 provides further details about the dataset collected.Table 4 shows samples of our dataset.

The Krippendorff’s for the inter- annotator agreement is 0.46 which is much higher than other hate speech datasets (Vigna et al. 2017; Ousidhoum et al.2019).Class labels: The class label (hateful, offensive, normal) of a post was decided based on majority voting.We found 919 cases where all the three annotators chose a different class.We did not consider these posts for our analysis.To decide the target community of a post, we rely on ma- jority voting.We consider that a target community is present 14869 Label is Normal Label is offensive/hate speech Replace attention value with 1/sentence length Take average of ground truth attentionGround truth attentionFinal ground truth attention Final ground truth attentionAnno 1 Anno 2 Anno 3Apply SoftmaxFigure 1: Ground truth attention.in the post, if at least two out of the three annotators have selected the target community.We also add a ﬁlter that the community should be present in at least 100 posts.

Based on this criteria, our dataset had the following ten communi- ties:African, Islam, Jewish, LGBTQ, Women, Refugee, Arab, Caucasian, Hispanic, Asian. The target community infor- mation would allow researchers to delve into issues related to bias in hate speech (Davidson, Bhattacharya, and Weber 2019).In our dataset, the top three communities that are tar- gets of hate speech are the African, Islam, and Jewish com- munity.In case of offensive speech, the top three targets areWomen, Africans, and LGBTQ.These observations are in agreement with previous research (Silva et al.2016).For the rationales’ annotation, each post that is labelled as hateful or offensive was further provided to the annota- tors7to highlight the rationales that could justify the ﬁnal class.Each post had rationale explanations provided by 2-3 annotators.We observe that the average number of tokens highlighted per post is 5.48 for offensive speech, and 5.47 for hate speech.

Average token per post in the whole dataset is 23.42. The top three content words in the hate speech ra- tionales are nigger, kike, and moslems, which are found in 30.02% of all the hateful posts.The top three content words for the offensive highlights are retarded, bitch, and white, which are found in 47.36% of all the offensive posts.Ground truth attention: In order to generate the ground truth attention for the post with hate speech/offensive label, we ﬁrst convert each rationale into an attention vector.This is a Boolean vector with length equal to the number of to- kens in the sentence.The tokens in the rationale are indi- cated by a value of 1 in the attention vector.Now we take the average of the these attention vectors to represent a com- mon ground truth attention vector for each post.The atten- tion vectors from the attention based models usually have their sum of elements equal to 1.

We normalize this com- mon attention vector through a softmax function to generate the ground truth attention. One issue with the ground truth attention vector could be that the difference between the val- ues of rationale and non-rationale tokens could be low.To handle this, we make use of the temperature parameter ( ) in the softmax function.This allows us to make the proba- bility distribution concentrate on the rationales.We tune this 7We tried to get the original annotator to highlight, however this was not always possible.Text Dad should have told the muzrat whore tofuck off, and went in anyway Label Hate Targets Islam Text A nigress too dumb to fuck has a scant chance of understanding anything beyond the size of a dick Label Hate Targets Women, African Text Twitter is full of tween dikes who think they’re superior because of “muh oppression.” News ﬂash: No one gives a shit.Label Offensive Targets LGBTQ Table 4: Examples from our dataset.

The bold portion of the text represents the annotator’s rationale. parameter using the validation set.Finally, if the label of the post is normal, we ignore the attention vectors and replace each element in the ground truth attention with 1/(sentence length) to represent uniform distribution.We illustrate this computation in Figure 1.Metrics For Evaluation Performance Based Metrics Following the standard practices, we report accuracy, macro F1-score, and AUROC score.These metrics would be able to evaluate the classiﬁer performance in distinguish- ing among the three classes, i.e., hate speech, offensive speech, and normal.Bias Based Metrics The hate speech detection models could make biased pre- dictions for particular groups who are already the target of such abuse (Sap et al.2019; Davidson, Bhattacharya, and Weber 2019).For example, the sentence “I love my niggas.” might be classiﬁed as hateful/offensive because of the asso- ciation of the word niggas with the black community.

These unintended identity-based bias could have negative impact on the target community. To measure such unintended model bias, we rely on the AUC based metrics developed by Borkan et al.(2019).These include Subgroup AUC, Back- ground Positive Subgroup Negative (BPSN) AUC, Back- ground Negative Subgroup Positive (BNSP) AUC, Gener- alized Mean of Bias AUCs.The task here is to classify the post as toxic (hate speech, offensive) ornot (normal).Here, the models will be evaluated on the grounds of how much they are able to reduce the unintended bias towards a target community (Borkan et al.2019).We restrict the evaluation to the test set only.By having this restriction, we are able to evaluate models in terms of bias reduction.Below, we brieﬂy describe each of the metrics.Subgroup AUC: Here, we select toxic and normal posts from the test set that mention the community under con- sideration.The ROC-AUC score of this set will provide us 14870 with the Subgroup AUC for a community.

This metric mea- sures the model’s ability to separate the toxic and normal comments in the context of the community (e.g., Asians, LGBTQ etc.). A higher value means that the model is do- ing a good job at distinguishing the toxic and normal posts speciﬁc to the community.BPSN (Background Positive, Subgroup Negative) AUC : Here, we select normal posts that mention the community and toxic posts that do not mention the community, from the test set.The ROC-AUC score of this set will provide us with the BPSN AUC for a community.This metric measures the false-positive rates of the model with respect to a commu- nity.A higher value means that a model is less likely to con- fuse between the normal post that mentions the community with a toxic post that does not.BNSP (Background Negative, Subgroup Positive) AUC : Here, we select toxic posts that mention the community and normal posts that do not mention the community, from the test set.

The ROC-AUC score for this set will provide us with the BNSP AUC for a community. The metric measures the false-negative rates of the model with respect to a com- munity.A higher value means that the model is less likely to confuse between a toxic post that mentions the community with a normal post without one.GMB (Generalized Mean of Bias) AUC: This metric was introduced by the Google Conversation AI Team as part of their Kaggle competition8.This metric combines the per- identify Bias AUCs into one overall measure as Mp(ms) = 1 NPN s=1mp s1 pwhere,Mp= thepthpower-mean function, ms= the bias metric mcalculated for subgroup sandN= number of identity subgroups (10).We use p= 5as was also done in the competition.We report the following three metrics for our dataset.-GMB-Subgroup-AUC: GMB AUC with Subgroup AUC as the bias metric.-GMB-BPSN-AUC: GMB AUC with BPSN AUC as the bias metric.-GMB-BNSP-AUC: GMB AUC with BNSP AUC as the bias metric.

Explainability Based Metrics We follow the framework in the ERASER benchmark by DeYoung et al. (2020) to measure the explainability as- pect of a model.We measure this using plausibility and faithfulness.Plausibility refers to how convincing the inter- pretation is to humans, while faithfulness refers to how accu- rately it reﬂects the true reasoning process of the model (Ja- covi and Goldberg 2020).For completeness, we explain the metrics brieﬂy below.Plausibility To measure the plausibility, we consider metrics for both discrete and soft selection.We report the IOU F1- Score and token F1-Score metric for the discrete case, and the AUPRC score for soft token selection (DeYoung et al.2020).8https://www.kaggle.com/c/jigsaw-unintended-bias-in- toxicity-classiﬁcation/overview/evaluationIntersection-Over-Union (IOU) permits credit assignment for partial matches.DeYoung et al.

(2020) deﬁnes IOU on a token level: for two spans, it is the size of the overlap of the tokens they cover divided by the size of their union. A pre- diction is considered as a match if the overlap with any of the ground truth rationales is more than 0.5.We use these partial matches to calculate an F1-score (IOU F1).We also mea- sure token-level precision and recall, and use these to derive token-level F1 scores (token F1).To measure the plausibil- ity for soft token scoring, we also report the Area Under the Precision-Recall curve (AUPRC) constructed by sweeping a threshold over the token scores.Faithfulness To measure the faithfulness, we report two metrics: comprehensiveness andsufﬁciency (DeYoung et al.2020).-Comprehensiveness: To measure comprehensiveness, we create a contrast example ~xi, for each post xi, where ~xiis calculated by removing the predicted rationales ri9 fromxi.Letm(xi)jbe the original prediction proba- bility provided by a model mfor the predicted class j.

Then we deﬁne m(xinri)jas the predicted probability of~xi(=xinri) by the model mfor the class j. We would expect the model prediction to be lower on re- moving the rationales.We can measure this as follows – comprehensiveness =m(xi)j m(xinri)j.A high value of comprehensiveness implies that the rationales were in- ﬂuential in the prediction.-Sufﬁciency measures the degree to which extracted ratio- nales are adequate for a model to make a prediction.We can measure this as follows – sufﬁciency =m(xi)j  m(ri)j.Model architectureSentence GT attentionPredicted attentionPossible with model having attention as output Predicted labelsGT labels Figure 2: Representation of the general model architecture showing how the attention of the model is trained using the ground truth (GT) attention.controls how much effect the attention loss has on the total loss.9We select the top 5 tokens as the rationales.The top 5 is se- lected as it is the avg.length of the annotation span in the dataset.

14871 Model [T oken Method] Perf ormance Bias Explainability Plausibility F aithfulness Acc." Macro F1 "AUROC" GMB-Sub . "GMB-BPSN" GMB-BNSP" IOU F1 "Token F1" AUPRC" Comp." Suff.

# CNN-GR U [LIME] 0.627 0.606 0.793 0.654 0.623 0.659 0.167 0.385 0.648 0.316 -0.082 BiRNN [LIME] 0.595 0.575 0.767 0.640 0.604 0.671 0.162 0.361 0.605 0.421 -0.051 BiRNN-Attn [Attn] 0.621 0.614 0.795 0.653 0.662 0.668 0.167 0.369 0.643 0.278 0.001 BiRNN-Attn [LIME] 0.621 0.614 0.795 0.653 0.662 0.668 0.162 0.386 0.650 0.308 -0.075 BiRNN- HateXplain [Attn] 0.629 0.629 0.805 0.691 0.636 0.674 0.222 0.506 0.841 0.281 0.039 BiRNN- HateXplain [LIME] 0.629 0.629 0.805 0.691 0.636 0.674 0.174 0.407 0.685 0.343 -0.075 BER T [Attn] 0.690 0.674 0.843 0.762 0.709 0.757 0.130 0.497 0.778 0.447 0.057 BER T [LIME] 0.690 0.674 0.843 0.762 0.709 0.757 0.118 0.468 0.747 0.436 0.008 BER T-HateXplain [Attn] 0.698 0.687 0.851 0.807 0.745 0.763 0.120 0.411 0.626 0.424 0.160 BER T-HateXplain [LIME] 0.698 0.687 0.851 0.807 0.745 0.763 0.112 0.452 0.722 0.500 0.004 Table 5: Model performance results. To select the tokens for explainability calculation, we used attention and LIME methods.

Model Details Each model has two versions, one where the models are trained using the ground truth class labels only (i.e., hate speech, offensive speech, and normal) and the other, where the models are trained using the ground truth attention and class labels, as shown in Figure 2. For training using the ground truth attention, the model needs to output some form of vector representing attention for each token according to the model, hence, the second version is not feasible for BiRNN and CNN-GRU models10.CNN-GRU Zhang, Robinson, and Tepper (2018) used CNN-GRU to achieve state-of-the-art for multiple hate speech datasets.We modify the original architecture to in- clude convolution 1D ﬁlters of window sizes 2, 3, 4 with each size having 100 ﬁlters.For the RNN part, we use GRU layer and ﬁnally max-pool the output representation from the hidden layers of the GRU architecture.This hidden layer is passed through a fully connected layer to ﬁnally output the prediction logits.

BiRNN For the BiRNN (Schuster and Paliwal 1997) model, we pass the tokens in the form of embeddings to a sequential model11. The last hidden state is passed through 2 fully connected layers.The output after that is used as the prediction logits.We use dropout layers after the embedding layer and before both the fully connected layers to regularise the trained model.BiRNN-Attention This model is identical to the BiRNN model but includes an attention layer (Liu and Lane 2016) after the sequential layer.This attention layer outputs an at- tention vector based on a context vector which is analogous to asking “which is the most important word?”.Weights from the attention vector are multiplied with the output hid- den units from the sequential layer and added to present a ﬁnal representation of the sentence.This representation is passed through two fully connected layers as in the BiRNN model.

Further to train the attention layer outputs, we com- pute cross entropy loss between the attention layer output and the ground truth attention (cf. Figure 1 for its computa- tion) as shown in Figure 2.10The limitation is due to the lack of an attention mechanism 11We experiment with LSTM and GRU.BERT BERT (Devlin et al.2019)12is a stack of trans- former encoder layers with 12 “attention heads”, i.e., fully connected neural networks augmented with a self attention mechanism.In order to ﬁne-tune BERT, we add a fully con- nected layer with the output corresponding to the CLS token in the input.This CLS token output usually holds the repre- sentation of the sentence.Next, to add attention supervision, we try to match the attention values corresponding to the CLS token in the ﬁnal layer to the ground truth attention, so that when the ﬁnal weighted representation of CLS is gen- erated, it would give attention to words as per the ground truth attention vector.

This is calculated using a cross en- tropy between the attention values and the ground truth at- tention vector as shown in Figure 2. Hyper-parameter Tuning All the methods are compared using the same train:development :test split of 8:1:1.We perform strat- iﬁed split on the dataset to maintain class balance.All the results are reported on the test set and the development set is used for hyper-parameter tuning.We use the common crawl13pre-trained GloVe embeddings (Pennington, Socher, and Manning 2014) to initialize the word embeddings for the non-BERT models.In our models, we set the token length to 128 for faster processing of the query14.We use Adam (Kingma and Ba 2015) optimizer and ﬁnd the learning rate to 0.001 for the non-BERT models and 2e-5 for BERT models using the development set.The RNN models prefer LSTM as the sequential layer with hidden layer size of 64 for BiRNN with attention and 128 for BiRNN.We use dropouts at different levels of the model.

The regulariser  controls how much effect the attention loss has on the total loss as in Figure 2. Optimum performance occurs with  being set to 100 for BiRNN with attention and BERT with attention in the supervised setting15.12We use the bert-base-uncased model having 12-layer, 768- hidden, 12-heads, 110M parameters.13840B tokens, 2.2M vocab, cased, 300d vectors.14Almost all the posts consist of less than 128 tokens in the data.15Please note that our selection of the best hyper-parameter was based on the performance, which is in lines with what is suggested in the literature.This dataset gives the ﬂexibility to choose best parameters based on plausibility and/or faithfulness, instead.14872 (a) SubGroup (b) BPSN (c) BNSP Figure 3: Community-wise results for each of the bias metrics.Results We report the main results obtained in Table 5.

Performance: We observe that models utilizing the hu- man rationales as part of the training (BiRNN-HateXplain [LIME & Attn], BERT-HateXplain [LIME & Attn]16) are able to perform slightly better in terms of the performance metrics. BiRNN-HateXplain [LIME & Attn] has improved score for all plausibitliy metrics and comprehensiveness as compared to BiRNN-Attn [LIME & Attn].In case of BERT- HateXplain [LIME], the faithfulness scores have improved as compared to other BERT models.However, the plausibil- ity scores have decreased.Bias: Similar to performance, models that utilize the human rationales as part of the training are able to perform better in reducing the unintended model bias for all the bias metrics.We observe that presence of community terms within the rationales is effective in reducing the unintended bias.We also looked at the model bias for each individual commu- nity in Figure 3.Figure 3a reports the community wise sub- group AUCROC.

We observe that while the GMB-Subgroup metric reports0.8 AUROC, the score for individual com- munity has large variations. Target communities like Asians have scores0.7, even for the best model.Communities like Hispanic seem to be biased toward having more false pos- itives.Models like BERT-HateXplain seem to be able to handle this bias much better than other models.Future re- search on hate speech, should consider the impact of the model performance on individual communities to have a clear understanding on the impact.Explainability: We observe that models such as BERT- HateXplain [LIME & Attn], which attain the best scores in terms of performance metrics and bias, do not perform well in terms of plausibility explainability metrics.In fact, BERT- HateXplain [Attn] has the worst score for sufﬁciency as compared to other models.

BERT-HateXplain [LIME] 16<model>-HateXplain denotes the models where we use su- pervised attention using ground truth attention vector.seems to be the best model for comprehensiveness metric. For plausibility metrics, we observe BiRNN-HateXplain [Attn] to have the best scores.For sufﬁciency, CNN-GRU seems to be doing the best.For the token method, LIME seems to be generating more faithful results as compared to attention.These are in agreement with DeYoung et al.(2020).Overall, we observe that a model’s performance met- ric alone is not enough.Models with slightly lower perfor- mance, but much higher scores for plausibility and faithful- ness might be preferred depending on the task at hand.The HateXplain dataset could be a valuable tool to analyze and develop models that provide more explainable results.Variations with : We measure the effect of on model per- formance (macro F1 and AUROC) and explainability (token F1, AUPRC, comp., and suff.).

We experiment with BiRNN- HateXplain [Attn] and BERT-HateXplain [Attn]. Increas- ing the value of improves the model performance, plaus- ability, and sufﬁcienty while degrading comprehensiveness.Limitations and Conclusion Our work has several limitations.First is the lack of external context.In our current models, we have not considered any external context such as proﬁle bio, user gender, history of posts etc., which might be helpful in the classiﬁcation task.Another issue is the focus on English language and lack of multilingual hate speech.In this paper, we have introduced HateXplain, a new benchmark dataset1for hate speech detection.The dataset consists of 20K posts from Gab and Twitter.Each data point is annotated with one of the hate/offensive/normal labels, target communities mentioned, and snippets (rationales) of the text marked by the annotators who support the label.

We test several state-of-the-art models on this dataset and per- form evaluation on several aspects of the hate speech detec- tion. Models that perform very well in classiﬁcation cannot always provide plausible and faithful rationales for their de- cisions.14873 References Aluru, S.S.; Mathew, B.; Saha, P.; and Mukherjee, A.2020.Deep Learning Models for Multilingual Hate Speech Detec- tion.arXiv preprint arXiv:2004.06465 .Arango, A.; P ´erez, J.; and Poblete, B.2019.Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation.In Proceedings of ACM SIGIR, 45–54.Paris, France.Basile, V .; Bosco, C.; Fersini, E.; Nozza, D.; Patti, V .; Rangel Pardo, F.M.; Rosso, P.; and Sanguinetti, M.2019.SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter.In Pro- ceedings of the 13th SemEval, 54–63.Minneapolis, Min- nesota, USA.Borkan, D.; Dixon, L.; Sorensen, J.; Thain, N.; and Vasser- man, L.2019.

Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classiﬁcation. In Companion of 2019 WWW, 491–500.San Francisco, CA, USA.Bosco, C.; Felice, D.; Poletto, F.; Sanguinetti, M.; and Mau- rizio, T.2018.Overview of the EV ALITA 2018 Hate Speech Detection Task.In EVALITA 2018, volume 2263, 1–9.Turin, Italy.Burnap, P.; and Williams, M.L.2016.Us and them: iden- tifying cyber hate on Twitter across multiple protected char- acteristics.EPJ Data Sci.5(1): 11.Camburu, O.; Rockt ¨aschel, T.; Lukasiewicz, T.; and Blun- som, P.2018.e-SNLI: Natural Language Inference with Natural Language Explanations.In NeurIPS, 9560–9572.Montr ´eal, Canada.Council, E.2016.EU Regulation 2016/679 General Data Protection Regulation (GDPR).Ofﬁcial Journal of the Eu- ropean Union 59(6): 1–88.Davidson, T.; Bhattacharya, D.; and Weber, I.2019.Racial Bias in Hate Speech and Abusive Language Detection Datasets.In Proceedings of the Third Workshop on Abusive Language Online, 25–35.Florence, Italy.

Davidson, T.; Warmsley, D.; Macy, M. W.; and Weber, I.2017.Automated Hate Speech Detection and the Problem of Offensive Language.In Proceedings of the 11th ICWSM, 512–515.Montr ´eal, Qu ´ebec, Canada.de Gibert, O.; Perez, N.; Garc ´ıa-Pablos, A.; and Cuadros, M.2018.Hate Speech Dataset from a White Supremacy Forum.InProceedings of the 2nd Workshop on Abusive Language Online), 11–20.Brussels, Belgium.Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K.2019.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.In Proceedings of 2019 NAACL , 4171–4186.Minneapolis, Minnesota.DeYoung, J.; Jain, S.; Rajani, N.F.; Lehman, E.; Xiong, C.; Socher, R.; and Wallace, B.C.2020.ERASER: A Bench- mark to Evaluate Rationalized NLP Models.In Proceedings of 58th ACL, 4443–4458.Online.Dixon, L.; Li, J.; Sorensen, J.; Thain, N.; and Vasserman, L.2018.Measuring and Mitigating Unintended Bias in Text Classiﬁcation.In Proceedings of 2018 AIES, 67–73.Doshi-Velez, Finale; Kim, B.

2017. Towards A Rigor- ous Science of Interpretable Machine Learning.In eprint arXiv:1702.08608.Fortuna, P.; and Nunes, S.2018.A survey on automatic detection of hate speech in text.ACM Computing Surveys (CSUR) 51(4): 85.Founta, A.; Djouvas, C.; Chatzakou, D.; Leontiadis, I.; Blackburn, J.; Stringhini, G.; Vakali, A.; Sirivianos, M.; and Kourtellis, N.2018.Large Scale Crowdsourcing and Char- acterization of Twitter Abusive Behavior.In Proceedings of 12th ICWSM, 491–500.Stanford, California, USA.Goodfellow, I.; Bengio, Y .; and Courville, A.2016.Deep Learning.MIT Press.http://www.deeplearningbook.org.Greenberg, J.; and Pyszczynski, T.1985.The effect of an overheard ethnic slur on evaluations of the target: How to spread a social disease.Journal of Experimental Social Psy- chology 21(1): 61–72.Gr¨ondahl, T.; Pajola, L.; Juuti, M.; Conti, M.; and Asokan, N.2018.All You Need is: Evading Hate Speech Detec- tion.In Proceedings of the 11th ACM AIS Workshop, 2–12.Toronto, ON, Canada.

Jacovi, A.; and Goldberg, Y . 2020.Towards Faithfully In- terpretable NLP Systems: How Should We Deﬁne and Eval- uate Faithfulness?In Proceedings of 58th ACL, 4198–4205.Online.Kingma, D.P.; and Ba, J.2015.Adam: A Method for Stochastic Optimization.In Proceedings of 3rd ICLR.San Diego, CA, USA.Lei, T.; Barzilay, R.; and Jaakkola, T.S.2016.Rationalizing Neural Predictions.In Proceedings of the 2016 EMNLP, 107–117.Austin, Texas, USA.Lima, L.; Reis, J.C.; Melo, P.; Murai, F.; Araujo, L.; Vikatos, P.; and Benevenuto, F.2018.Inside the right- leaning echo chambers: Characterizing gab, an unmoder- ated social system.In 2018 IEEE/ACM ASONAM, 515–522.Barcelona, Spain.Lipton, Z.C.2018.The mythos of model interpretability.Communications of the ACM 61(10): 36–43.Liu, B.; and Lane, I.2016.Attention-Based Recurrent Neu- ral Network Models for Joint Intent Detection and Slot Fill- ing.In 17th Interspeech, 685–689.San Francisco, CA, USA.Mathew, B.; Dutt, R.; Goyal, P.; and Mukherjee, A.2019a.

Spread of hate speech in online social media. In Proceedings of 10th WebSci, 173–182.Boston, MA, USA.Mathew, B.; Illendula, A.; Saha, P.; Sarkar, S.; Goyal, P.; and Mukherjee, A.2020.Hate Begets Hate: A Temporal Study of Hate Speech.Proceedings of ACM CSCW .Mathew, B.; Saha, P.; Tharad, H.; Rajgaria, S.; Singhania, P.; Maity, S.K.; Goyal, P.; and Mukherjee, A.2019b.Thou shalt not hate: Countering online hate speech.In Proceed- ings of ICWSM, 369–380.Munich, Germany.Mishra, P.; Del Tredici, M.; Yannakoudakis, H.; and Shutova, E.2018.Author proﬁling for abuse detection.In 14874 Proceedings of 27th ICCL, 1088–1098.Santa Fe, New Mex- ico, USA.Olteanu, A.; Castillo, C.; Boy, J.; and Varshney, K.R.2018.The Effect of Extremist Violence on Hateful Speech Online.InProceedings of 12th ICWSM, 221–230.Stanford, Califor- nia, USA.Ousidhoum, N.; Lin, Z.; Zhang, H.; Song, Y .; and Yeung, D.- Y .2019.Multilingual and Multi-Aspect Hate Speech Anal- ysis.In Proceedings of 2019 EMNLP-IJCNLP, 4667–4676.

Hong Kong, China. Pennington, J.; Socher, R.; and Manning, C.D.2014.Glove: Global Vectors for Word Representation.In Proceedings of 2014 EMNLP, 1532–1543.Doha, Qatar.Qian, J.; Bethke, A.; Liu, Y .; Belding, E.M.; and Wang, W.Y .2019a.A Benchmark Dataset for Learning to In- tervene in Online Hate Speech.In Proceedings of 2019 EMNLP, 4754–4763.Hong Kong, China.Qian, J.; ElSherief, M.; Belding, E.M.; and Wang, W.Y .2018a.Hierarchical CV AE for Fine-Grained Hate Speech Classiﬁcation.In Proceedings of 2018 EMNLP, 3550–3559.Brussels, Belgium.Qian, J.; ElSherief, M.; Belding, E.M.; and Wang, W.Y .2018b.Leveraging Intra-User and Inter-User Represen- tation Learning for Automated Hate Speech Detection.InProceedings of 2018 NAACL, 118–123.New Orleans, Louisiana, USA.Qian, J.; ElSherief, M.; Belding, E.M.; and Wang, W.Y .2019b.Learning to Decipher Hate Symbols.In Proceedings of 2019 NAACL, 3006–3015.Minneapolis, MN, USA.Rajani, N.F.; McCann, B.; Xiong, C.; and Socher, R.2019.Explain Yourself!

Leveraging Language Models for Com- monsense Reasoning. In Proceedings of 57th ACL, 4932– 4942.Florence, Italy.Ribeiro, M.H.; Calais, P.H.; Santos, Y .A.; Almeida, V .A.F.; and Jr., W.M.2018.Characterizing and Detecting Hateful Users on Twitter.In Proceedings of 12th ICWSM, 676–679.Stanford, California, USA.Ribeiro, M.T.; Singh, S.; and Guestrin, C.2016.”Why Should I Trust You?”: Explaining the Predictions of Any Classiﬁer.In Proceedings of the 22nd KDD, 1135–1144.New York, NY , USA.Sanguinetti, M.; Poletto, F.; Bosco, C.; Patti, V .; and Stranisci, M.2018.An Italian Twitter Corpus of Hate Speech against Immigrants.In Proceedings of 11th LREC.Miyazaki, Japan.Sap, M.; Card, D.; Gabriel, S.; Choi, Y .; and Smith, N.A.2019.The Risk of Racial Bias in Hate Speech Detection.In Proceedings of 57th ACL, 1668–1678.Florence, Italy.Schuster, M.; and Paliwal, K.K.1997.Bidirectional recur- rent neural networks.IEEE Trans.Signal Process.45(11): 2673–2681.Silva, L.

A.; Mondal, M.; Correa, D.; Benevenuto, F.; and Weber, I. 2016.Analyzing the Targets of Hate in Online Social Media.In Proceedings of 10th ICWSM, 687–690.Cologne, Germany.Soral, W.; Bilewicz, M.; and Winiewski, M.2018.Exposure to hate speech increases prejudice through desensitization.Aggressive behavior 44(2): 136–146.Van Huynh, T.; Nguyen, V .D.; Van Nguyen, K.; Nguyen, N.L.-T.; and Nguyen, A.G.-T.2019.Hate Speech Detection on Vietnamese Social Media Text using the Bi-GRU-LSTM- CNN Model.arXiv preprint arXiv:1911.03644 .Vigna, F.D.; Cimino, A.; Dell’Orletta, F.; Petrocchi, M.; and Tesconi, M.2017.Hate Me, Hate Me Not: Hate Speech De- tection on Facebook.In Proceedings of 1st Italian Confer- ence on Cybersecurity, volume 1816, 86–95.Venice, Italy.Wang, W.; Chen, L.; Thirunarayan, K.; and Sheth, A.P.2014.Cursing in English on twitter.In Proceedings of 17th ACM CSCW, 415–425.Baltimore, MD, USA.Waseem, Z.; and Hovy, D.2016.Hateful Symbols or Hate- ful People?

Predictive Features for Hate Speech Detection on Twitter. In Proceedings of the NAACL Student Research Workshop, 88–93.San Diego, California, USA.Williams, M.L.; Burnap, P.; Javed, A.; Liu, H.; and Ozalp, S.2020.Hate in the machine: anti-Black and anti-Muslim social media posts as predictors of ofﬂine racially and reli- giously aggravated crime.The British Journal of Criminol- ogy60(1): 93–117.Yessenalina, A.; Choi, Y .; and Cardie, C.2010.Automat- ically Generating Annotator Rationales to Improve Senti- ment Classiﬁcation.In Proceedings of the 48th ACL, 336– 341.Uppsala, Sweden.Zaidan, O.; Eisner, J.; and Piatko, C.D.2007.Using “Anno- tator Rationales” to Improve Machine Learning for Text Cat- egorization.In Proceedings of NAACL, 260–267.Rochester, New York, USA.Zannettou, S.; Bradlyn, B.; Cristofaro, E.D.; Kwak, H.; Siri- vianos, M.; Stringhini, G.; and Blackburn, J.2018.What is Gab: A Bastion of Free Speech or an Alt-Right Echo Cham- ber.In Companion of WWW, 1007–1014.

Lyon , France. Zhang, Z.; Robinson, D.; and Tepper, J.A.2018.Detecting Hate Speech on Twitter Using a Convolution-GRU Based Deep Neural Network.In Proceedings of The Semantic Web, 745–760.Heraklion, Crete, Greece.14875

HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection Binny Mathew1*, Punyajoy Saha1*, Seid Muhie Yimam2 Chris Biemann2, Pawan Goyal1, Animesh Mukherjee1 1Indian Institute of Technology, Kharagpur, India 2Universit ¨at Hamburg, Germany binnymathew@iitkgp.ac.in, punyajoys@iitkgp.ac.in, yimam@informatik.uni-hamburg.de biemann@informatik.uni-hamburg.de, pawang@cse.iitkgp.ac.in, animeshm@cse.iitkgp.ac.in Abstract Hate speech is a challenging issue plaguing the online so- cial media.While better models for hate speech detection are continuously being developed, there is little research on the bias andinterpretability aspects of hate speech.In this paper, we introduce HateXplain, the ﬁrst benchmark hate speech dataset covering multiple aspects of the issue.

Each post in our dataset is annotated from three different perspectives: the basic, commonly used 3-class classiﬁcation (i.e., hate, offen- sive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. We utilize existing state-of-the-art models and ob- serve that even models that perform very well in classiﬁcation do not score high on explainability metrics like model plau- sibility andfaithfulness.We also observe that models, which utilize the human rationales for training, perform better in re- ducing unintended bias towards target communities.We have made our code and dataset public1for other researchers2.Introduction The increase in online hate speech is a major cultural threat, as it already resulted in crime against minorities, see e.g.(Williams et al.2020).

To tackle this issue, there has been a rising interest in hate speech detection to expose and regulate this phenomenon. Several hate speech datasets (Ousidhoum et al.2019; Qian et al.2019b; de Gibert et al.2018; Sanguinetti et al.2018), models (Zhang, Robinson, and Tepper 2018; Mishra et al.2018; Qian et al.2018b,a), and shared tasks (Basile et al.2019; Bosco et al.2018) have been made available in the recent years by the community, towards the development of automatic hate speech detection.While many models have claimed to achieve state-of- the-art performance on some datasets, they fail to gener- alize (Arango, P ´erez, and Poblete 2019; Gr ¨ondahl et al.*Equal Contribution Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org).All rights reserved.1https://github.com/punyajoy/HateXplain 2Disclaimer: The article contains material that many will ﬁnd offensive or hateful; however this cannot be avoided owing to the nature of the work.2018).

The models may classify comments that refer to cer- tain commonly-attacked identities (e.g., gay, black, muslim) as toxic without the comment having any intention of be- ing toxic (Dixon et al. 2018; Borkan et al.2019).A large prior on certain trigger vocabulary leads to biased predic- tions that may discriminate against particular groups who are already the target of such abuse (Sap et al.2019; David- son, Bhattacharya, and Weber 2019).Another issue with the current methods is the lack of explanation about the deci- sions made.With hate speech detection models becoming increasingly complex, it is getting difﬁcult to explain their decisions (Goodfellow, Bengio, and Courville 2016).Laws such as General Data Protection Regulation (GDPR (Coun- cil 2016)) in Europe have recently established a “right to ex- planation”.This calls for a shift in perspective from perfor- mance based models to interpretable models.

In our work, we approach model explainability by learning the target classiﬁcation and the reasons for the human decision jointly, and also to their mutual improvement. We therefore have compiled a dataset that covers mul- tiple aspects of hate speech.We collect posts from Twit- ter3and Gab4, and ask Amazon Mechanical Turk (MTurk) workers to annotate these posts to cover three facets.In ad- dition to classifying each post into hate, offensive, or nor- mal speech, annotators are asked to select the target com- munities mentioned in the post.Subsequently, the annota- tors are asked to highlight parts of the text that could jus- tify their classiﬁcation decision5.The notion of justiﬁcation, here modeled as ‘human attention’, is very broad with many possible realizations (Lipton 2018; Doshi-Velez 2017).In this paper, we speciﬁcally focus on using rationales, i.e., snippets of text from a source text that support a particular categorization.

Such rationales have been used in common- sense explanations (Rajani et al. 2019), e-SNLI (Camburu et al.2018) and several other tasks (DeYoung et al.2020).If these rationales are good reasons for decisions, then mod- els guided towards these in training could be made more human-decision-taking-like.Consider the examples in Table 1.The ﬁrst row shows 3https://twitter.com/ 4https://gab.com/ 5In case the post is classiﬁed as normal, the annotators does not need to highlight any span.TheThirty-FifthAAAIConferenceonArtificial Intelligence(AAAI-21) 14867 Model Text Label Human Annotator The jews are again using holohoax as an excuse to spread their agenda .Hilter should have eradicated them HS CNN-GRU The jews are again using holohoax asan excuse to spread their agenda .

Hilter should have eradicated them HS BiRNN Thejews areagain using holohoax as an excuse to spread their agenda .Hilter should have eradicated them HS BiRNN-Attn The jews are again using holohoax as an excuse tospread their agenda . Hilter should have eradicated them HS BiRNN-HateXplain The jews are again using holohoax as an excuse to spread their agenda .Hilter should have eradicated them HS BERT Thejews are again using holohoax as an excuse to spread their agenda .Hilter should have eradicated them OF BERT-HateXplain The jews areagain using holohoax as an excuse to spread their agenda .Hilter should have eradicated them OF Table 1: Example of the rationales predicted by different models compared to human annotators.The bold part marks tokens that the human annotator and model found important for the prediction.The underlined part marks tokens which the model found important, but the human annotators did not.

the tokens (‘rationales’) that were selected by human an- notators which they believe are important for the classiﬁ- cation. The next six rows show the important tokens (using LIME (Ribeiro, Singh, and Guestrin 2016)), which helped various models in the classiﬁcation.We observe that even when the model is making the correct prediction (hate speech – HS in this case), the reason (‘rationales’) for this varies across models.In case of BERT, we observe that it at- tends to several of the tokens that human annotators deemed important, but assigns the wrong label (offensive speech - OF).In summary, we introduce HateXplain, the ﬁrst bench- mark dataset for hate speech with word and phrase level span annotations that capture human rationales for the label- ing.Using MTurk, we collect a large dataset of around 20K posts and annotate them to cover three aspects of each post.

We use several models on this dataset and observe that while they show a good model performance, they do not fare well in terms of model interpretability/explainability. We also ob- serve that providing these rationales as input during training helps in improving a model’s performance and reducing the unintended bias.We believe that this dataset would serve as a fundamental source for the future hate speech research.Related Works Hate Speech The public expression of hate speech affects the deval- uation of minority members (Greenberg and Pyszczynski 1985) and such frequent and repetitive exposure to hate speech could increase an individual’s outgroup prejudice (Soral, Bilewicz, and Winiewski 2018).Real world violent events could also lead to increased hate speech in online space (Olteanu et al.2018).To tackle this, various methods have been proposed for hate speech detection (Burnap and Williams 2016; Ribeiro et al.2018; Zhang, Robinson, and Tepper 2018; Qian et al.2018a).

The recent interest in hate speech research has led to the release of datasets in multiple languages (Ousidhoum et al. 2019; Sanguinetti et al.2018) along with different computational approaches to combat online hate (Qian et al.2019a; Mathew et al.2019b; Aluru et al.2020).A recurrent issue with the majority of previous research is that many of them tend to conﬂate hate speech and abu-sive/offensive6language (Davidson et al.2017).Some of the works have combined offensive and hate language under a single concept, while very few works, such as (Davidson et al.2017; Founta et al.2018) and Van Huynh et al.(2019) have attempted to separate offensive from hate speech.We argue that this, although subjective, is an important aspect as there are lots of messages that are offensive but do not qual- ify as hate speech.For example, consider the word ‘nigga’.The word is used everyday in online language by the African American community (Vigna et al.2017).

Similarly, words likehoeandbitch are used commonly in rap lyrics. Such language is prevalent on social media (Wang et al.2014) and any hate speech detection system should include these for the system to be usable.To this end, we have assumed that a given text can belong to one of the three classes: hate, offensive, normal.We have adopted the classes based on the work of Davidson et al.(2017).Table 2 provides a compari- son between some hate speech datasets.Explainability/Interpretability Zaidan, Eisner, and Piatko (2007) introduced the concept of using rationales, in which human annotators would high- light a span of text that could support their labeling decision.The authors utilized these enriched rationale annotation on a smaller set of training data, which helped to improve sen- timent classiﬁcation.Yessenalina, Choi, and Cardie (2010) built on this work and developed methods that automatically generate rationales.

Lei, Barzilay, and Jaakkola (2016) also proposed an encoder-generator framework, which provides quality rationales without any annotations. In our paper, we utilize the concept of rationales and pro- vide the ﬁrst benchmark hate speech dataset with human level explanations.We have made our model and dataset public1for other researchers.Dataset Collection And Annotation Strategies In this section, we provide the annotation strategies we have followed, the dataset selection approaches used, and the statistics of the collected dataset.6We have used the terms offensive and abusive interchangeably in our paper as they are arguably very similar (Founta et al.2018).14868 Dataset Labels Total Size Language Target Labels?Rationales?Waseem and Hovy (2016) racist, sexist, normal 16,914 English   Davidson et al.(2017) Hate Speech, Offensive, Normal 24,802 English   Founta et al.(2018) Abusive, Hateful, Normal, Spam 80,000 English   Ousidhoum et al.

(2019) Labels for ﬁve different aspects 13,000 English, French, Arabic X  HateXplain (Ours) Hate Speech, Offensive, Normal 20,148 English X X Table 2: Comparison of different hate speech datasets. Dataset Sampling We collect our dataset from sources where previous stud- ies on hate speech have been conducted: Twitter (Davidson et al.2017; Fortuna and Nunes 2018) and Gab (Lima et al.2018; Mathew et al.2020; Zannettou et al.2018).Follow- ing the existing literature, we build a corpus of posts (tweets and gab posts) using lexicons.We combined the lexicon set provided by Davidson et al.(2017), Ousidhoum et al.(2019), and Mathew et al.(2019a) to generate a single lexi- con.For Twitter, we ﬁlter the tweets from the 1% randomly collected tweets in the time period Jan-2019 to Jun-2020.In case of Gab, we use the dataset provided by Mathew et al.(2019a).We do not consider reposts and remove duplicates.

We also ensure that the posts do not contain links, pictures, or videos as they indicate additional information that might not be available to the annotators. However, we do not ex- clude the emojis from the text as they might carry important information for the hate and offensive speech labeling task.The posts were anonymized by replacing the usernames with <user> token.Annotation Procedure We use Amazon Mechanical Turk (MTurk) workers for our annotation task.Each post in our dataset contains three types of annotations.First, whether the text is a hate speech, of- fensive speech, or normal.Second, the target communities in the text.Third, if the text is considered as hate speech, or offensive by majority of the annotators, we further ask the annotators to annotate parts of the text, which are words or phrases that could be a potential reason for the given anno- tation.These additional span annotations allow us to further explore how hate or offensive speech manifests itself.

Target Group Annotation The primary goal of the an- notation task is to determine whether a given text is hate- ful, offensive, or neither of the two, i.e. normal.As noted above, we also get span annotations as reasons for the label assigned to a post (hateful or offensive).To further enrich the dataset, we ask the workers to decide the groups that the hate/offensive speech is targeting.We included target groups based on Race, Religion, Gender, Sexual Orientation etc.Annotation Instructions And Design Of The Interface Before starting the annotation task, workers are explicitly warned that the annotation task displays some hateful or offensive content.We prepare instructions for workers that clearly explain the goal of the annotation task, how to anno- tate spans and also include a deﬁnition for each category.

We provided multiple examples with classiﬁcation, target com-Twitter Gab Total Hateful 708 5,227 5,935 Offensive 2,328 3,152 5,480 Normal 5,770 2,044 7,814 Undecided 249 670 919 Total 9,055 11,093 20,148 Table 3: Dataset details. “Undecided” refers to the cases where all the three annotators chose a different class.munity and span annotations to help the annotators under- stand the task.To further ensure high quality dataset, we use built-in MTurk qualiﬁcation requirements, namely the HIT Approval Rate (95%) for all Requesters’ HITs and the Num- ber of HITs Approved (5,000) requirements.Dataset Creation Steps For the dataset creation, we ﬁrst conducted a pilot annotation study followed by the main annotation task.Pilot annotation: In the pilot task, each annotator was provided with 20 posts and they were required to do the hate/offensive speech classiﬁcation as well as identify the target community (if any).

In order to have a clear under- standing of the task, they were provided with multiple exam- ples along with explanations for the labelling process. The main purpose of the pilot task was to shortlist those anno- tators who were able to do the classiﬁcation accurately.We also collected feedback from annotators to improve the main annotation task.A total of 621 annotators took part in the pi- lot task.Out of these, 253 were selected for the main task.Main annotation: After the pilot annotation, once we had ascertained the quality of the annotators, we started with the main annotation task.In each round, we would select a batch of around 200 posts.Each post was annotated by three annotators, then majority voting was applied to decide the ﬁnal label.The ﬁnal dataset is composed of 9,055 posts from Twitter and 11,093 posts from Gab.Table 3 provides further details about the dataset collected.Table 4 shows samples of our dataset.

The Krippendorff’s for the inter- annotator agreement is 0.46 which is much higher than other hate speech datasets (Vigna et al. 2017; Ousidhoum et al.2019).Class labels: The class label (hateful, offensive, normal) of a post was decided based on majority voting.We found 919 cases where all the three annotators chose a different class.We did not consider these posts for our analysis.To decide the target community of a post, we rely on ma- jority voting.We consider that a target community is present 14869 Label is Normal Label is offensive/hate speech Replace attention value with 1/sentence length Take average of ground truth attentionGround truth attentionFinal ground truth attention Final ground truth attentionAnno 1 Anno 2 Anno 3Apply SoftmaxFigure 1: Ground truth attention.in the post, if at least two out of the three annotators have selected the target community.We also add a ﬁlter that the community should be present in at least 100 posts.

Based on this criteria, our dataset had the following ten communi- ties:African, Islam, Jewish, LGBTQ, Women, Refugee, Arab, Caucasian, Hispanic, Asian. The target community infor- mation would allow researchers to delve into issues related to bias in hate speech (Davidson, Bhattacharya, and Weber 2019).In our dataset, the top three communities that are tar- gets of hate speech are the African, Islam, and Jewish com- munity.In case of offensive speech, the top three targets areWomen, Africans, and LGBTQ.These observations are in agreement with previous research (Silva et al.2016).For the rationales’ annotation, each post that is labelled as hateful or offensive was further provided to the annota- tors7to highlight the rationales that could justify the ﬁnal class.Each post had rationale explanations provided by 2-3 annotators.We observe that the average number of tokens highlighted per post is 5.48 for offensive speech, and 5.47 for hate speech.

Average token per post in the whole dataset is 23.42. The top three content words in the hate speech ra- tionales are nigger, kike, and moslems, which are found in 30.02% of all the hateful posts.The top three content words for the offensive highlights are retarded, bitch, and white, which are found in 47.36% of all the offensive posts.Ground truth attention: In order to generate the ground truth attention for the post with hate speech/offensive label, we ﬁrst convert each rationale into an attention vector.This is a Boolean vector with length equal to the number of to- kens in the sentence.The tokens in the rationale are indi- cated by a value of 1 in the attention vector.Now we take the average of the these attention vectors to represent a com- mon ground truth attention vector for each post.The atten- tion vectors from the attention based models usually have their sum of elements equal to 1.

We normalize this com- mon attention vector through a softmax function to generate the ground truth attention. One issue with the ground truth attention vector could be that the difference between the val- ues of rationale and non-rationale tokens could be low.To handle this, we make use of the temperature parameter ( ) in the softmax function.This allows us to make the proba- bility distribution concentrate on the rationales.We tune this 7We tried to get the original annotator to highlight, however this was not always possible.Text Dad should have told the muzrat whore tofuck off, and went in anyway Label Hate Targets Islam Text A nigress too dumb to fuck has a scant chance of understanding anything beyond the size of a dick Label Hate Targets Women, African Text Twitter is full of tween dikes who think they’re superior because of “muh oppression.” News ﬂash: No one gives a shit.Label Offensive Targets LGBTQ Table 4: Examples from our dataset.

The bold portion of the text represents the annotator’s rationale. parameter using the validation set.Finally, if the label of the post is normal, we ignore the attention vectors and replace each element in the ground truth attention with 1/(sentence length) to represent uniform distribution.We illustrate this computation in Figure 1.Metrics For Evaluation Performance Based Metrics Following the standard practices, we report accuracy, macro F1-score, and AUROC score.These metrics would be able to evaluate the classiﬁer performance in distinguish- ing among the three classes, i.e., hate speech, offensive speech, and normal.Bias Based Metrics The hate speech detection models could make biased pre- dictions for particular groups who are already the target of such abuse (Sap et al.2019; Davidson, Bhattacharya, and Weber 2019).For example, the sentence “I love my niggas.” might be classiﬁed as hateful/offensive because of the asso- ciation of the word niggas with the black community.

These unintended identity-based bias could have negative impact on the target community. To measure such unintended model bias, we rely on the AUC based metrics developed by Borkan et al.(2019).These include Subgroup AUC, Back- ground Positive Subgroup Negative (BPSN) AUC, Back- ground Negative Subgroup Positive (BNSP) AUC, Gener- alized Mean of Bias AUCs.The task here is to classify the post as toxic (hate speech, offensive) ornot (normal).Here, the models will be evaluated on the grounds of how much they are able to reduce the unintended bias towards a target community (Borkan et al.2019).We restrict the evaluation to the test set only.By having this restriction, we are able to evaluate models in terms of bias reduction.Below, we brieﬂy describe each of the metrics.Subgroup AUC: Here, we select toxic and normal posts from the test set that mention the community under con- sideration.The ROC-AUC score of this set will provide us 14870 with the Subgroup AUC for a community.

This metric mea- sures the model’s ability to separate the toxic and normal comments in the context of the community (e.g., Asians, LGBTQ etc.). A higher value means that the model is do- ing a good job at distinguishing the toxic and normal posts speciﬁc to the community.BPSN (Background Positive, Subgroup Negative) AUC : Here, we select normal posts that mention the community and toxic posts that do not mention the community, from the test set.The ROC-AUC score of this set will provide us with the BPSN AUC for a community.This metric measures the false-positive rates of the model with respect to a commu- nity.A higher value means that a model is less likely to con- fuse between the normal post that mentions the community with a toxic post that does not.BNSP (Background Negative, Subgroup Positive) AUC : Here, we select toxic posts that mention the community and normal posts that do not mention the community, from the test set.

The ROC-AUC score for this set will provide us with the BNSP AUC for a community. The metric measures the false-negative rates of the model with respect to a com- munity.A higher value means that the model is less likely to confuse between a toxic post that mentions the community with a normal post without one.GMB (Generalized Mean of Bias) AUC: This metric was introduced by the Google Conversation AI Team as part of their Kaggle competition8.This metric combines the per- identify Bias AUCs into one overall measure as Mp(ms) = 1 NPN s=1mp s1 pwhere,Mp= thepthpower-mean function, ms= the bias metric mcalculated for subgroup sandN= number of identity subgroups (10).We use p= 5as was also done in the competition.We report the following three metrics for our dataset.-GMB-Subgroup-AUC: GMB AUC with Subgroup AUC as the bias metric.-GMB-BPSN-AUC: GMB AUC with BPSN AUC as the bias metric.-GMB-BNSP-AUC: GMB AUC with BNSP AUC as the bias metric.

Explainability Based Metrics We follow the framework in the ERASER benchmark by DeYoung et al. (2020) to measure the explainability as- pect of a model.We measure this using plausibility and faithfulness.Plausibility refers to how convincing the inter- pretation is to humans, while faithfulness refers to how accu- rately it reﬂects the true reasoning process of the model (Ja- covi and Goldberg 2020).For completeness, we explain the metrics brieﬂy below.Plausibility To measure the plausibility, we consider metrics for both discrete and soft selection.We report the IOU F1- Score and token F1-Score metric for the discrete case, and the AUPRC score for soft token selection (DeYoung et al.2020).8https://www.kaggle.com/c/jigsaw-unintended-bias-in- toxicity-classiﬁcation/overview/evaluationIntersection-Over-Union (IOU) permits credit assignment for partial matches.DeYoung et al.

(2020) deﬁnes IOU on a token level: for two spans, it is the size of the overlap of the tokens they cover divided by the size of their union. A pre- diction is considered as a match if the overlap with any of the ground truth rationales is more than 0.5.We use these partial matches to calculate an F1-score (IOU F1).We also mea- sure token-level precision and recall, and use these to derive token-level F1 scores (token F1).To measure the plausibil- ity for soft token scoring, we also report the Area Under the Precision-Recall curve (AUPRC) constructed by sweeping a threshold over the token scores.Faithfulness To measure the faithfulness, we report two metrics: comprehensiveness andsufﬁciency (DeYoung et al.2020).-Comprehensiveness: To measure comprehensiveness, we create a contrast example ~xi, for each post xi, where ~xiis calculated by removing the predicted rationales ri9 fromxi.Letm(xi)jbe the original prediction proba- bility provided by a model mfor the predicted class j.

Then we deﬁne m(xinri)jas the predicted probability of~xi(=xinri) by the model mfor the class j. We would expect the model prediction to be lower on re- moving the rationales.We can measure this as follows – comprehensiveness =m(xi)j m(xinri)j.A high value of comprehensiveness implies that the rationales were in- ﬂuential in the prediction.-Sufﬁciency measures the degree to which extracted ratio- nales are adequate for a model to make a prediction.We can measure this as follows – sufﬁciency =m(xi)j  m(ri)j.Model architectureSentence GT attentionPredicted attentionPossible with model having attention as output Predicted labelsGT labels Figure 2: Representation of the general model architecture showing how the attention of the model is trained using the ground truth (GT) attention.controls how much effect the attention loss has on the total loss.9We select the top 5 tokens as the rationales.The top 5 is se- lected as it is the avg.length of the annotation span in the dataset.

14871 Model [T oken Method] Perf ormance Bias Explainability Plausibility F aithfulness Acc." Macro F1 "AUROC" GMB-Sub . "GMB-BPSN" GMB-BNSP" IOU F1 "Token F1" AUPRC" Comp." Suff.

# CNN-GR U [LIME] 0.627 0.606 0.793 0.654 0.623 0.659 0.167 0.385 0.648 0.316 -0.082 BiRNN [LIME] 0.595 0.575 0.767 0.640 0.604 0.671 0.162 0.361 0.605 0.421 -0.051 BiRNN-Attn [Attn] 0.621 0.614 0.795 0.653 0.662 0.668 0.167 0.369 0.643 0.278 0.001 BiRNN-Attn [LIME] 0.621 0.614 0.795 0.653 0.662 0.668 0.162 0.386 0.650 0.308 -0.075 BiRNN- HateXplain [Attn] 0.629 0.629 0.805 0.691 0.636 0.674 0.222 0.506 0.841 0.281 0.039 BiRNN- HateXplain [LIME] 0.629 0.629 0.805 0.691 0.636 0.674 0.174 0.407 0.685 0.343 -0.075 BER T [Attn] 0.690 0.674 0.843 0.762 0.709 0.757 0.130 0.497 0.778 0.447 0.057 BER T [LIME] 0.690 0.674 0.843 0.762 0.709 0.757 0.118 0.468 0.747 0.436 0.008 BER T-HateXplain [Attn] 0.698 0.687 0.851 0.807 0.745 0.763 0.120 0.411 0.626 0.424 0.160 BER T-HateXplain [LIME] 0.698 0.687 0.851 0.807 0.745 0.763 0.112 0.452 0.722 0.500 0.004 Table 5: Model performance results. To select the tokens for explainability calculation, we used attention and LIME methods.

Model Details Each model has two versions, one where the models are trained using the ground truth class labels only (i.e., hate speech, offensive speech, and normal) and the other, where the models are trained using the ground truth attention and class labels, as shown in Figure 2. For training using the ground truth attention, the model needs to output some form of vector representing attention for each token according to the model, hence, the second version is not feasible for BiRNN and CNN-GRU models10.CNN-GRU Zhang, Robinson, and Tepper (2018) used CNN-GRU to achieve state-of-the-art for multiple hate speech datasets.We modify the original architecture to in- clude convolution 1D ﬁlters of window sizes 2, 3, 4 with each size having 100 ﬁlters.For the RNN part, we use GRU layer and ﬁnally max-pool the output representation from the hidden layers of the GRU architecture.This hidden layer is passed through a fully connected layer to ﬁnally output the prediction logits.

BiRNN For the BiRNN (Schuster and Paliwal 1997) model, we pass the tokens in the form of embeddings to a sequential model11. The last hidden state is passed through 2 fully connected layers.The output after that is used as the prediction logits.We use dropout layers after the embedding layer and before both the fully connected layers to regularise the trained model.BiRNN-Attention This model is identical to the BiRNN model but includes an attention layer (Liu and Lane 2016) after the sequential layer.This attention layer outputs an at- tention vector based on a context vector which is analogous to asking “which is the most important word?”.Weights from the attention vector are multiplied with the output hid- den units from the sequential layer and added to present a ﬁnal representation of the sentence.This representation is passed through two fully connected layers as in the BiRNN model.

Further to train the attention layer outputs, we com- pute cross entropy loss between the attention layer output and the ground truth attention (cf. Figure 1 for its computa- tion) as shown in Figure 2.10The limitation is due to the lack of an attention mechanism 11We experiment with LSTM and GRU.BERT BERT (Devlin et al.2019)12is a stack of trans- former encoder layers with 12 “attention heads”, i.e., fully connected neural networks augmented with a self attention mechanism.In order to ﬁne-tune BERT, we add a fully con- nected layer with the output corresponding to the CLS token in the input.This CLS token output usually holds the repre- sentation of the sentence.Next, to add attention supervision, we try to match the attention values corresponding to the CLS token in the ﬁnal layer to the ground truth attention, so that when the ﬁnal weighted representation of CLS is gen- erated, it would give attention to words as per the ground truth attention vector.

This is calculated using a cross en- tropy between the attention values and the ground truth at- tention vector as shown in Figure 2. Hyper-parameter Tuning All the methods are compared using the same train:development :test split of 8:1:1.We perform strat- iﬁed split on the dataset to maintain class balance.All the results are reported on the test set and the development set is used for hyper-parameter tuning.We use the common crawl13pre-trained GloVe embeddings (Pennington, Socher, and Manning 2014) to initialize the word embeddings for the non-BERT models.In our models, we set the token length to 128 for faster processing of the query14.We use Adam (Kingma and Ba 2015) optimizer and ﬁnd the learning rate to 0.001 for the non-BERT models and 2e-5 for BERT models using the development set.The RNN models prefer LSTM as the sequential layer with hidden layer size of 64 for BiRNN with attention and 128 for BiRNN.We use dropouts at different levels of the model.

The regulariser  controls how much effect the attention loss has on the total loss as in Figure 2. Optimum performance occurs with  being set to 100 for BiRNN with attention and BERT with attention in the supervised setting15.12We use the bert-base-uncased model having 12-layer, 768- hidden, 12-heads, 110M parameters.13840B tokens, 2.2M vocab, cased, 300d vectors.14Almost all the posts consist of less than 128 tokens in the data.15Please note that our selection of the best hyper-parameter was based on the performance, which is in lines with what is suggested in the literature.This dataset gives the ﬂexibility to choose best parameters based on plausibility and/or faithfulness, instead.14872 (a) SubGroup (b) BPSN (c) BNSP Figure 3: Community-wise results for each of the bias metrics.Results We report the main results obtained in Table 5.

Performance: We observe that models utilizing the hu- man rationales as part of the training (BiRNN-HateXplain [LIME & Attn], BERT-HateXplain [LIME & Attn]16) are able to perform slightly better in terms of the performance metrics. BiRNN-HateXplain [LIME & Attn] has improved score for all plausibitliy metrics and comprehensiveness as compared to BiRNN-Attn [LIME & Attn].In case of BERT- HateXplain [LIME], the faithfulness scores have improved as compared to other BERT models.However, the plausibil- ity scores have decreased.Bias: Similar to performance, models that utilize the human rationales as part of the training are able to perform better in reducing the unintended model bias for all the bias metrics.We observe that presence of community terms within the rationales is effective in reducing the unintended bias.We also looked at the model bias for each individual commu- nity in Figure 3.Figure 3a reports the community wise sub- group AUCROC.

We observe that while the GMB-Subgroup metric reports0.8 AUROC, the score for individual com- munity has large variations. Target communities like Asians have scores0.7, even for the best model.Communities like Hispanic seem to be biased toward having more false pos- itives.Models like BERT-HateXplain seem to be able to handle this bias much better than other models.Future re- search on hate speech, should consider the impact of the model performance on individual communities to have a clear understanding on the impact.Explainability: We observe that models such as BERT- HateXplain [LIME & Attn], which attain the best scores in terms of performance metrics and bias, do not perform well in terms of plausibility explainability metrics.In fact, BERT- HateXplain [Attn] has the worst score for sufﬁciency as compared to other models.

BERT-HateXplain [LIME] 16<model>-HateXplain denotes the models where we use su- pervised attention using ground truth attention vector.seems to be the best model for comprehensiveness metric. For plausibility metrics, we observe BiRNN-HateXplain [Attn] to have the best scores.For sufﬁciency, CNN-GRU seems to be doing the best.For the token method, LIME seems to be generating more faithful results as compared to attention.These are in agreement with DeYoung et al.(2020).Overall, we observe that a model’s performance met- ric alone is not enough.Models with slightly lower perfor- mance, but much higher scores for plausibility and faithful- ness might be preferred depending on the task at hand.The HateXplain dataset could be a valuable tool to analyze and develop models that provide more explainable results.Variations with : We measure the effect of on model per- formance (macro F1 and AUROC) and explainability (token F1, AUPRC, comp., and suff.).

We experiment with BiRNN- HateXplain [Attn] and BERT-HateXplain [Attn]. Increas- ing the value of improves the model performance, plaus- ability, and sufﬁcienty while degrading comprehensiveness.Limitations and Conclusion Our work has several limitations.First is the lack of external context.In our current models, we have not considered any external context such as proﬁle bio, user gender, history of posts etc., which might be helpful in the classiﬁcation task.Another issue is the focus on English language and lack of multilingual hate speech.In this paper, we have introduced HateXplain, a new benchmark dataset1for hate speech detection.The dataset consists of 20K posts from Gab and Twitter.Each data point is annotated with one of the hate/offensive/normal labels, target communities mentioned, and snippets (rationales) of the text marked by the annotators who support the label.

We test several state-of-the-art models on this dataset and per- form evaluation on several aspects of the hate speech detec- tion. Models that perform very well in classiﬁcation cannot always provide plausible and faithful rationales for their de- cisions.14873 References Aluru, S.S.; Mathew, B.; Saha, P.; and Mukherjee, A.2020.Deep Learning Models for Multilingual Hate Speech Detec- tion.arXiv preprint arXiv:2004.06465 .Arango, A.; P ´erez, J.; and Poblete, B.2019.Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation.In Proceedings of ACM SIGIR, 45–54.Paris, France.Basile, V .; Bosco, C.; Fersini, E.; Nozza, D.; Patti, V .; Rangel Pardo, F.M.; Rosso, P.; and Sanguinetti, M.2019.SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter.In Pro- ceedings of the 13th SemEval, 54–63.Minneapolis, Min- nesota, USA.Borkan, D.; Dixon, L.; Sorensen, J.; Thain, N.; and Vasser- man, L.2019.

Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classiﬁcation. In Companion of 2019 WWW, 491–500.San Francisco, CA, USA.Bosco, C.; Felice, D.; Poletto, F.; Sanguinetti, M.; and Mau- rizio, T.2018.Overview of the EV ALITA 2018 Hate Speech Detection Task.In EVALITA 2018, volume 2263, 1–9.Turin, Italy.Burnap, P.; and Williams, M.L.2016.Us and them: iden- tifying cyber hate on Twitter across multiple protected char- acteristics.EPJ Data Sci.5(1): 11.Camburu, O.; Rockt ¨aschel, T.; Lukasiewicz, T.; and Blun- som, P.2018.e-SNLI: Natural Language Inference with Natural Language Explanations.In NeurIPS, 9560–9572.Montr ´eal, Canada.Council, E.2016.EU Regulation 2016/679 General Data Protection Regulation (GDPR).Ofﬁcial Journal of the Eu- ropean Union 59(6): 1–88.Davidson, T.; Bhattacharya, D.; and Weber, I.2019.Racial Bias in Hate Speech and Abusive Language Detection Datasets.In Proceedings of the Third Workshop on Abusive Language Online, 25–35.Florence, Italy.

Davidson, T.; Warmsley, D.; Macy, M. W.; and Weber, I.2017.Automated Hate Speech Detection and the Problem of Offensive Language.In Proceedings of the 11th ICWSM, 512–515.Montr ´eal, Qu ´ebec, Canada.de Gibert, O.; Perez, N.; Garc ´ıa-Pablos, A.; and Cuadros, M.2018.Hate Speech Dataset from a White Supremacy Forum.InProceedings of the 2nd Workshop on Abusive Language Online), 11–20.Brussels, Belgium.Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K.2019.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.In Proceedings of 2019 NAACL , 4171–4186.Minneapolis, Minnesota.DeYoung, J.; Jain, S.; Rajani, N.F.; Lehman, E.; Xiong, C.; Socher, R.; and Wallace, B.C.2020.ERASER: A Bench- mark to Evaluate Rationalized NLP Models.In Proceedings of 58th ACL, 4443–4458.Online.Dixon, L.; Li, J.; Sorensen, J.; Thain, N.; and Vasserman, L.2018.Measuring and Mitigating Unintended Bias in Text Classiﬁcation.In Proceedings of 2018 AIES, 67–73.Doshi-Velez, Finale; Kim, B.

2017. Towards A Rigor- ous Science of Interpretable Machine Learning.In eprint arXiv:1702.08608.Fortuna, P.; and Nunes, S.2018.A survey on automatic detection of hate speech in text.ACM Computing Surveys (CSUR) 51(4): 85.Founta, A.; Djouvas, C.; Chatzakou, D.; Leontiadis, I.; Blackburn, J.; Stringhini, G.; Vakali, A.; Sirivianos, M.; and Kourtellis, N.2018.Large Scale Crowdsourcing and Char- acterization of Twitter Abusive Behavior.In Proceedings of 12th ICWSM, 491–500.Stanford, California, USA.Goodfellow, I.; Bengio, Y .; and Courville, A.2016.Deep Learning.MIT Press.http://www.deeplearningbook.org.Greenberg, J.; and Pyszczynski, T.1985.The effect of an overheard ethnic slur on evaluations of the target: How to spread a social disease.Journal of Experimental Social Psy- chology 21(1): 61–72.Gr¨ondahl, T.; Pajola, L.; Juuti, M.; Conti, M.; and Asokan, N.2018.All You Need is: Evading Hate Speech Detec- tion.In Proceedings of the 11th ACM AIS Workshop, 2–12.Toronto, ON, Canada.

Jacovi, A.; and Goldberg, Y . 2020.Towards Faithfully In- terpretable NLP Systems: How Should We Deﬁne and Eval- uate Faithfulness?In Proceedings of 58th ACL, 4198–4205.Online.Kingma, D.P.; and Ba, J.2015.Adam: A Method for Stochastic Optimization.In Proceedings of 3rd ICLR.San Diego, CA, USA.Lei, T.; Barzilay, R.; and Jaakkola, T.S.2016.Rationalizing Neural Predictions.In Proceedings of the 2016 EMNLP, 107–117.Austin, Texas, USA.Lima, L.; Reis, J.C.; Melo, P.; Murai, F.; Araujo, L.; Vikatos, P.; and Benevenuto, F.2018.Inside the right- leaning echo chambers: Characterizing gab, an unmoder- ated social system.In 2018 IEEE/ACM ASONAM, 515–522.Barcelona, Spain.Lipton, Z.C.2018.The mythos of model interpretability.Communications of the ACM 61(10): 36–43.Liu, B.; and Lane, I.2016.Attention-Based Recurrent Neu- ral Network Models for Joint Intent Detection and Slot Fill- ing.In 17th Interspeech, 685–689.San Francisco, CA, USA.Mathew, B.; Dutt, R.; Goyal, P.; and Mukherjee, A.2019a.

Spread of hate speech in online social media. In Proceedings of 10th WebSci, 173–182.Boston, MA, USA.Mathew, B.; Illendula, A.; Saha, P.; Sarkar, S.; Goyal, P.; and Mukherjee, A.2020.Hate Begets Hate: A Temporal Study of Hate Speech.Proceedings of ACM CSCW .Mathew, B.; Saha, P.; Tharad, H.; Rajgaria, S.; Singhania, P.; Maity, S.K.; Goyal, P.; and Mukherjee, A.2019b.Thou shalt not hate: Countering online hate speech.In Proceed- ings of ICWSM, 369–380.Munich, Germany.Mishra, P.; Del Tredici, M.; Yannakoudakis, H.; and Shutova, E.2018.Author proﬁling for abuse detection.In 14874 Proceedings of 27th ICCL, 1088–1098.Santa Fe, New Mex- ico, USA.Olteanu, A.; Castillo, C.; Boy, J.; and Varshney, K.R.2018.The Effect of Extremist Violence on Hateful Speech Online.InProceedings of 12th ICWSM, 221–230.Stanford, Califor- nia, USA.Ousidhoum, N.; Lin, Z.; Zhang, H.; Song, Y .; and Yeung, D.- Y .2019.Multilingual and Multi-Aspect Hate Speech Anal- ysis.In Proceedings of 2019 EMNLP-IJCNLP, 4667–4676.

Hong Kong, China. Pennington, J.; Socher, R.; and Manning, C.D.2014.Glove: Global Vectors for Word Representation.In Proceedings of 2014 EMNLP, 1532–1543.Doha, Qatar.Qian, J.; Bethke, A.; Liu, Y .; Belding, E.M.; and Wang, W.Y .2019a.A Benchmark Dataset for Learning to In- tervene in Online Hate Speech.In Proceedings of 2019 EMNLP, 4754–4763.Hong Kong, China.Qian, J.; ElSherief, M.; Belding, E.M.; and Wang, W.Y .2018a.Hierarchical CV AE for Fine-Grained Hate Speech Classiﬁcation.In Proceedings of 2018 EMNLP, 3550–3559.Brussels, Belgium.Qian, J.; ElSherief, M.; Belding, E.M.; and Wang, W.Y .2018b.Leveraging Intra-User and Inter-User Represen- tation Learning for Automated Hate Speech Detection.InProceedings of 2018 NAACL, 118–123.New Orleans, Louisiana, USA.Qian, J.; ElSherief, M.; Belding, E.M.; and Wang, W.Y .2019b.Learning to Decipher Hate Symbols.In Proceedings of 2019 NAACL, 3006–3015.Minneapolis, MN, USA.Rajani, N.F.; McCann, B.; Xiong, C.; and Socher, R.2019.Explain Yourself!

Leveraging Language Models for Com- monsense Reasoning. In Proceedings of 57th ACL, 4932– 4942.Florence, Italy.Ribeiro, M.H.; Calais, P.H.; Santos, Y .A.; Almeida, V .A.F.; and Jr., W.M.2018.Characterizing and Detecting Hateful Users on Twitter.In Proceedings of 12th ICWSM, 676–679.Stanford, California, USA.Ribeiro, M.T.; Singh, S.; and Guestrin, C.2016.”Why Should I Trust You?”: Explaining the Predictions of Any Classiﬁer.In Proceedings of the 22nd KDD, 1135–1144.New York, NY , USA.Sanguinetti, M.; Poletto, F.; Bosco, C.; Patti, V .; and Stranisci, M.2018.An Italian Twitter Corpus of Hate Speech against Immigrants.In Proceedings of 11th LREC.Miyazaki, Japan.Sap, M.; Card, D.; Gabriel, S.; Choi, Y .; and Smith, N.A.2019.The Risk of Racial Bias in Hate Speech Detection.In Proceedings of 57th ACL, 1668–1678.Florence, Italy.Schuster, M.; and Paliwal, K.K.1997.Bidirectional recur- rent neural networks.IEEE Trans.Signal Process.45(11): 2673–2681.Silva, L.

A.; Mondal, M.; Correa, D.; Benevenuto, F.; and Weber, I. 2016.Analyzing the Targets of Hate in Online Social Media.In Proceedings of 10th ICWSM, 687–690.Cologne, Germany.Soral, W.; Bilewicz, M.; and Winiewski, M.2018.Exposure to hate speech increases prejudice through desensitization.Aggressive behavior 44(2): 136–146.Van Huynh, T.; Nguyen, V .D.; Van Nguyen, K.; Nguyen, N.L.-T.; and Nguyen, A.G.-T.2019.Hate Speech Detection on Vietnamese Social Media Text using the Bi-GRU-LSTM- CNN Model.arXiv preprint arXiv:1911.03644 .Vigna, F.D.; Cimino, A.; Dell’Orletta, F.; Petrocchi, M.; and Tesconi, M.2017.Hate Me, Hate Me Not: Hate Speech De- tection on Facebook.In Proceedings of 1st Italian Confer- ence on Cybersecurity, volume 1816, 86–95.Venice, Italy.Wang, W.; Chen, L.; Thirunarayan, K.; and Sheth, A.P.2014.Cursing in English on twitter.In Proceedings of 17th ACM CSCW, 415–425.Baltimore, MD, USA.Waseem, Z.; and Hovy, D.2016.Hateful Symbols or Hate- ful People?

Predictive Features for Hate Speech Detection on Twitter. In Proceedings of the NAACL Student Research Workshop, 88–93.San Diego, California, USA.Williams, M.L.; Burnap, P.; Javed, A.; Liu, H.; and Ozalp, S.2020.Hate in the machine: anti-Black and anti-Muslim social media posts as predictors of ofﬂine racially and reli- giously aggravated crime.The British Journal of Criminol- ogy60(1): 93–117.Yessenalina, A.; Choi, Y .; and Cardie, C.2010.Automat- ically Generating Annotator Rationales to Improve Senti- ment Classiﬁcation.In Proceedings of the 48th ACL, 336– 341.Uppsala, Sweden.Zaidan, O.; Eisner, J.; and Piatko, C.D.2007.Using “Anno- tator Rationales” to Improve Machine Learning for Text Cat- egorization.In Proceedings of NAACL, 260–267.Rochester, New York, USA.Zannettou, S.; Bradlyn, B.; Cristofaro, E.D.; Kwak, H.; Siri- vianos, M.; Stringhini, G.; and Blackburn, J.2018.What is Gab: A Bastion of Free Speech or an Alt-Right Echo Cham- ber.In Companion of WWW, 1007–1014.

Lyon , France. Zhang, Z.; Robinson, D.; and Tepper, J.A.2018.Detecting Hate Speech on Twitter Using a Convolution-GRU Based Deep Neural Network.In Proceedings of The Semantic Web, 745–760.Heraklion, Crete, Greece.14875

